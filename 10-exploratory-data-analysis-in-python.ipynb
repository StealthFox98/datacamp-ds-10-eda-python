{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4d6b278",
   "metadata": {},
   "source": [
    "# Chapter 10. Exploratory Data Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6990c60f",
   "metadata": {},
   "source": [
    "Exploratory Data Analysis, or EDA for short, is the process of cleaning and reviewing data to derive insights such as descriptive statistics and correlation and generate hypotheses for experiments.\n",
    "\n",
    "EDA results often inform the next steps for the dataset, whether that be generating hypotheses, preparing the data for use in a machine learning model, or even throwing the data out and gathering new data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf9eebd",
   "metadata": {},
   "source": [
    "# 10.1 Getting to know a datset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b9cdd",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783e81e",
   "metadata": {},
   "source": [
    "### A first look with ``.head()``\n",
    "\n",
    "- We'll import the books data from a csv file using pd.read_csv and save it as a DataFrame called \"books\".\n",
    "- Taking a look at the top of the DataFrame using the head function, we can see that our data contains columns representing book names, authors, ratings, publishing years, and genres.\n",
    "\n",
    "```\n",
    "books = pd.read_csv(\"books.csv\")\n",
    "books.head()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8602d6d",
   "metadata": {},
   "source": [
    "### Gathering more ``.info()``\n",
    "\n",
    "- pandas also offers a quick way to summarize the number of missing values in each column, the data type of each column, and memory usage using the ``.info`` method.\n",
    "\n",
    "```\n",
    "books.info()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3588cc",
   "metadata": {},
   "source": [
    "### Closer look at categorical columns\n",
    "\n",
    "- A common question about categorical columns in a dataset is how many data points we have in each category.\n",
    "- We can select the genre column and use the pandas Series method ``.value_counts()`` to find the number of books with each genre.\n",
    "\n",
    "```\n",
    "books.value_counts(\"genre\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbe9ae3",
   "metadata": {},
   "source": [
    "### ``.describe()`` numerical columns\n",
    "\n",
    "- Gaining a quick understanding of data included in numerical columns is done with the help of the DataFrame.describe method.\n",
    "- Calling ``.describe`` on books, we see that it returns the count, mean, and standard deviation of the values in each numerical column (in this case rating and year), along with the min, max, and quartile values.\n",
    "\n",
    "```\n",
    "books.describe()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadc2a53",
   "metadata": {},
   "source": [
    "### Visualizing numerical data\n",
    "\n",
    "- Histograms are a classic way to look at the distribution of numerical data by splitting numerical values into discrete bins and visualizing the count of values in each bin.\n",
    "- To create a histogram, we'll use sns.histplot and pass the books DataFrame as the data argument.\n",
    "\n",
    "```\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.histplot(data=books, x=\"rating\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56955d6e",
   "metadata": {},
   "source": [
    "#### 1. Adjusting bin width\n",
    "\n",
    "- We can set a bin width of 0.1 using the binwidth keyword argument.\n",
    "- It will depend on the data's value range\n",
    "\n",
    "```\n",
    "sns.histplot(data=books, x=\"rating\", binwidth=0.1)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a27054",
   "metadata": {},
   "source": [
    "## Data validation\n",
    "\n",
    "Data validation is an important early step in EDA. We want to understand whether data types and ranges are as expected before we progress too far in our analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0e531",
   "metadata": {},
   "source": [
    "### Validating data types\n",
    "\n",
    "- ``.info()`` gives a quick overview of data types included in a dataset along with other information such as the number of non-missing values.\n",
    "- We can also use the DataFrame ``.dtypes`` attribute if we're only interested in data types.\n",
    "\n",
    "```\n",
    "books.info()\n",
    "books.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3b1f2d",
   "metadata": {},
   "source": [
    "### Updating data types\n",
    "\n",
    "- ``.astype()`` function allows us to change data types without too much effort.\n",
    "- We redefine the year column by selecting the column and calling the ``.astype()`` method, indicating we'd like to change the column to an integer.\n",
    "- Then we use the ``.dtypes`` attribute to check that the year column data is now stored as integers\n",
    "\n",
    "```\n",
    "books[\"year\"] = books[\"year\"].astype(int)\n",
    "books.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50358c19",
   "metadata": {},
   "source": [
    "### Validating categorical data\n",
    "\n",
    "- We can validate categorical data by comparing values in a column to a list of expected values using ``.isin()``, which can either be applied to a Series as we'll show here or to an entire DataFrame.\n",
    "- The function returns a Series/DataFrame of the same size and shape as the original but with True and False in place of all values, depending on whether the value from the original Series/DataFrame was included in the list passed to ``.isin()``.\n",
    "\n",
    "```\n",
    "books[\"genre\"].isin([\"Fiction\", \"Non Fiction\"]) # books whose genres are Fiction and Non Fiction\n",
    "~books[\"genre\"].isin([\"Fiction\", \"Non Fiction\"]) # books that do not belong to the Fiction and Non Fiction genres\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f157fd",
   "metadata": {},
   "source": [
    "### Validating numerical data\n",
    "\n",
    "- We can select and view only the numerical columns in a DataFrame by calling the ``select_dtypes()`` method and passing ``\"number\"`` as the argument.\n",
    "- Check the lowest and highest years by using the ``.min()`` and ``.max()`` functions, respectively.\n",
    "- View a more detailed picture of the distribution of year data using Seaborn's ``boxplot()`` function.\n",
    "- View the year data grouped by a categorical variable such as genre by setting the ``y`` keyword argument.\n",
    "\n",
    "```\n",
    "books.select_dtypes(\"number\").head()\n",
    "books[\"year\"].min()\n",
    "books[\"year\"].max()\n",
    "\n",
    "sns.boxplot(data=books, x=\"year\")\n",
    "plt.show()\n",
    "\n",
    "sns.boxplot(data=books, x=\"year\", y=\"genre\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5946658",
   "metadata": {},
   "source": [
    "## Data summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e87fd0b",
   "metadata": {},
   "source": [
    "### Exploring groups of data\n",
    "\n",
    "- We can explore the characteristics of subsets of data further with the help of the ``.groupby`` function, which groups data by a given category, allowing the user to chain an aggregating function like ``.mean()`` or ``.count()`` to describe the data within each group. \n",
    "\n",
    "```\n",
    "books.groupby(\"genre\").mean()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e1731b",
   "metadata": {},
   "source": [
    "### Aggregating functions\n",
    "\n",
    "- Sum: `.sum()`\n",
    "- Count: `.count()`\n",
    "- Minimum: `.min()`\n",
    "- Maximum: `.max()`\n",
    "- Variance: `.var()`\n",
    "- Standard deviation: `.std()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588cf0f9",
   "metadata": {},
   "source": [
    "### Aggregating ungrouped data\n",
    "\n",
    "- The ``.agg()`` function, short for aggregate, applis aggreagating functions across a DataFrame.\n",
    "\n",
    "```\n",
    "books.agg([\"mean\", \"std\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b6704a",
   "metadata": {},
   "source": [
    "### Specifying aggregations for columns\n",
    "\n",
    "- We can even use a dictionary to specify which aggregation functions to apply to which columns.\n",
    "- The keys in the dictionary are the columns to apply the aggregation, and each value is a list of the specific aggregating functions to apply to that column.\n",
    "\n",
    "```\n",
    "books.agg({\"rating\": [\"mean\", \"std\"], \"year\": [\"median\"]})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbf1bce",
   "metadata": {},
   "source": [
    "### Named summary columns\n",
    "\n",
    "- By combining ``.agg()`` and ``.groupby()``, we can apply these new exploration skills to grouped data.\n",
    "- We can create named columns with our desired aggregations by using the ``.agg()`` function and creating named tuples inside it.\n",
    "- Each named tuple should include a column name followed by the aggregating function to apply to that column.\n",
    "\n",
    "```\n",
    "books.groupby(\"genre\").agg(\n",
    "    mean_rating=(\"rating\", \"mean\"),\n",
    "    std_rating=(\"rating\", \"std\"),\n",
    "    median_year=(\"year\", \"median\")\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb9193c",
   "metadata": {},
   "source": [
    "### Visualizing categorical summaries\n",
    "\n",
    "- We can display similar information visually using a barplot.\n",
    "- In Seaborn, bar plots will automatically calculate the mean of a quantitative variable like rating across grouped categorical data.\n",
    "- Bar plots also show a 95% confidence interval for the mean as a vertical line on the top of each bar.\n",
    "\n",
    "```\n",
    "sns.barplot(data=books, x=\"genre\", y=\"rating\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0ee9d3",
   "metadata": {},
   "source": [
    "# 10.2 Data cleaning and imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648b4b84",
   "metadata": {},
   "source": [
    "## Addressing missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc7b5a2",
   "metadata": {},
   "source": [
    "### Why is missing data a problem?\n",
    "\n",
    "1. Affects distributions\n",
    "    - Missing heights of taller students\n",
    "2. Less representative of the population\n",
    "    - Certain groups disproportionately represented\n",
    "3. Can result in drawing incorrect conlcusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5d3e6",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "\n",
    "- We can count the number of missing values per column by chaining the ``.isna()`` and ``.sum()`` methods.\n",
    "- ``isna`` refers to the fact that missing values are represented as na in DataFrames.\n",
    "\n",
    "```\n",
    "print(salaries.isna().sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d8a91",
   "metadata": {},
   "source": [
    "### Strategies for addresing missing data\n",
    "\n",
    "There are various approaches to handle missing data.\n",
    "\n",
    "- **Drop missing values (if they ammount 5% or less of total values)**\n",
    "- **Impute mean, median, mode (depends on distirbution and context)**\n",
    "- **Impute by sub-group (i.e. different experience levels have different median salary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa0d2b",
   "metadata": {},
   "source": [
    "#### 1. Dropping missing values\n",
    "\n",
    "- To calculate our missing values threshold we multiply the length of our DataFrame by five percent\n",
    "- We can use Boolean indexing to filter for columns with missing values less than or equal to this threshold, storing them as a variable called ``cols_to_drop``.\n",
    "- We drop missing values by calling ``.dropna()``, passing ``cols_to_drop`` to the subset argument. We set ``inplace=True`` so the DataFrame is updated.\n",
    "\n",
    "```\n",
    "threshold = len(salaries) * 0.05\n",
    "print(threshold)\n",
    "cols_to_drop = salaries.columns[salaries.isna().sum() <= threshold]\n",
    "print(cols_to_drop)\n",
    "salaries.dropna(subset=cols_to_drop, inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2565f579",
   "metadata": {},
   "source": [
    "#### 2. Imputing a summary statistic and Checking the remaining values\n",
    "\n",
    "- We then filter for the remaining columns with missing values.\n",
    "- To impute the mode for the first three columns, we loop through them and call the ``.fillna()`` method, passing the respective column's ``mode()`` and indexing the first item, which contains the mode, in square brackets.\n",
    "\n",
    "```\n",
    "cols_with_missing_values = salaries.columns[salaries.isna().sum() > 0]\n",
    "print(cols_with_missing_values)\n",
    "for col in cols_with_missing_values[:-1]: # [:-1] means \"all elements of the sequence but the last\"\n",
    "    salaries[col].fillna(salaries[col].mode()[0])\n",
    "print(salaries.isna().sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ba0523",
   "metadata": {},
   "source": [
    "#### 3. Imputing by sub-group\n",
    "\n",
    "- We use the ``.to_dict()`` method, storing the grouped data as a dictionary.\n",
    "- i.e. Printing the dictionary returns the median salary for each experience level\n",
    "- We then impute using the ``.fillna()`` method, providing the Experience column and calling the ``.map()`` method\n",
    "\n",
    "```\n",
    "salaries_dict = salaries.groupby(\"Experience\")[\"Salary_USD\"].median().to_dict()\n",
    "print(salaries_dict)\n",
    "salaries[\"Salary_USD\"] = salaries[\"Salary_USD\"].fillna(salaries[\"Experience\"].map(salaries_dict))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73c3d8b",
   "metadata": {},
   "source": [
    "## Converting and analyzing categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e561b",
   "metadata": {},
   "source": [
    "### Previewing the data\n",
    "\n",
    "- We can use the ``select_dtypes()`` method to filter any non-numeric data.\n",
    "- Chaining ``.head()`` allows us to preview these columns in our salaries DataFrame\n",
    "\n",
    "```\n",
    "print(salaries.select_dtypes(\"object\").head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26243d0",
   "metadata": {},
   "source": [
    "### Examining a column: frequency of values and number of unique values\n",
    "\n",
    "- Output is truncated by pandas automatically if it is too much information.\n",
    "- Also, we can count how many unique values there are using pandas ``.nunique()`` method.\n",
    "\n",
    "```\n",
    "print(salaries[\"Designation\"].value_counts()) # how many times each unique value appears in the column.\n",
    "print(salaries[\"Designation\"].nunique()) # gives you the count of unique values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724a95b",
   "metadata": {},
   "source": [
    "### Extracting value from categories\n",
    "\n",
    "- Current format limits our ability to generate insights\n",
    "- We can use pandas .str.contains() method, which allows to search for a column for a specific string or multiple strings\n",
    "\n",
    "```\n",
    "salaires[\"Designation\"].str.contains(\"Scientist\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbdc88",
   "metadata": {},
   "source": [
    "### Finding multiple phrases in strings\n",
    "\n",
    "- Filter for rows containing one or more phrases\n",
    "- We use the string-dot-contains method again, but this time we include a pipe between our two phrases.\n",
    "- This will return True if an observation in the Designation column contains (i.e.) Machine Learning or AI, or false if neither of these phrases are present\n",
    "\n",
    "```\n",
    "salaires[\"Designation\"].str.contains(\"Machine learning|AI\")\n",
    "```\n",
    "\n",
    "- Filter for job titles that start with a specific phrase\n",
    "\n",
    "```\n",
    "salaires[\"Designation\"].str.contains(\"^Data\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ea094d",
   "metadata": {},
   "source": [
    "### Creating a new column with results from finding multiple phrases in strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f5931",
   "metadata": {},
   "source": [
    "#### 1. Creating a list for the values of our new column\n",
    "\n",
    "- We start by creating a list with the different categories of data roles, which will become the values of a new column in our DataFrame.\n",
    "\n",
    "```\n",
    "job_categories = [\"Data science\", \"Data Analytics\", \"Data engineering\", \"Machine learning\", \"Managerial\", \"Consultant\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97c1f0",
   "metadata": {},
   "source": [
    "#### 2. Create variables with filters\n",
    "\n",
    "- We then need to create variables containing our filters.\n",
    "\n",
    "```\n",
    "data_science = \"Data Scientist|NLP\"\n",
    "data_analyst = \"Analyst|Analytics\"\n",
    "data_engineer = \"Data Engineer|ETL|Architect|Infrastructure\"\n",
    "ml_engineer = \"Machine Learning|ML|Big Data|AI\"\n",
    "manager = \"Manager|Head|Director|Lead|Principal|Staff\"\n",
    "consultant = \"Consultant|Freelance\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cf6f37",
   "metadata": {},
   "source": [
    "#### 3. Create a list with range of conditions\n",
    "\n",
    "- The next step is to create a list with our range of conditions for the ``string.contains`` method.\n",
    "\n",
    "```\n",
    "conditions = [\n",
    "    (salaries[\"Designations\"].str.contains(data_science)),\n",
    "    (salaries[\"Designations\"].str.contains(data_analyst)),\n",
    "    (salaries[\"Designations\"].str.contains(data_engineer)),\n",
    "    (salaries[\"Designations\"].str.contains(ml_engineer)),\n",
    "    (salaries[\"Designations\"].str.contains(manager)),\n",
    "    (salaries[\"Designations\"].str.contains(consultant))\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e26af4",
   "metadata": {},
   "source": [
    "#### 4. Create a new column with results\n",
    "\n",
    "- Finally, we can create our new Job_Category column by using NumPy's ``np.select()`` function.\n",
    "    - It takes a list of conditions as the first argument\n",
    "    - Then a list of arrays to search for the conditions in\n",
    "    - By using an argument called ``default``, we tell NumPy to assign \"Other\" when a value in our conditions list is not found.\n",
    "\n",
    "```\n",
    "salaries[\"Job_Category\"] = np.select(\n",
    "    conditions,\n",
    "    job_categories,\n",
    "    default=\"Other\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a834b260",
   "metadata": {},
   "source": [
    "#### 5. Previewing job categories\n",
    "\n",
    "```\n",
    "print(salaries[[\"Designation\", \"Job_Category\"]].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba3eaa6",
   "metadata": {},
   "source": [
    "#### 6. Visualizing results frequency\n",
    "\n",
    "```\n",
    "sns.countplot(data=salaries, x=\"Job_Category\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf5b60",
   "metadata": {},
   "source": [
    "## Working with numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38c656a",
   "metadata": {},
   "source": [
    "### Converting strings to numbers\n",
    "\n",
    "- To remove commas, we can use the pandas Series .str.replace() method.\n",
    "- We first pass the characters we want to remove, followed by the characters to replace them with.\n",
    "- As we don't want to add characters back in, when we update the column we provide an empty string in this part of the method.\n",
    "- Printing the first five rows of this column, we see the commas have been removed.\n",
    "- We update the data type to float.\n",
    "\n",
    "```\n",
    "pd.Series.str.replace(\"characters to remove\", \"characters ot replace them\")\n",
    "salaries[\"Salary_In_Rupees\"] = salaries[\"Salary_In_Rupees\"].str.replace(\",\", \"\")\n",
    "print(salaries[\"Salary_In_Rupees\"].head())\n",
    "salaries[\"Salary_In_Rupees\"] = salaries[\"Salary_In_Rupees\"].astype(float)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771d97c4",
   "metadata": {},
   "source": [
    "### Adding summary statistics into a DataFrame\n",
    "\n",
    "- Sometimes we might prefer to add summary statistics directly into our DataFrame, rather than creating a summary table.\n",
    "- Example: Create a new column containing the standard deviation of Salary_USD, where values are conditional based on the Experience column.\n",
    "\n",
    "```\n",
    "salaries[\"std_dev\"] = salaries.groupby(\"Experience\")[\"Salary_USD\"].transform(lambda x: x.std())\n",
    "print(salaries[[\"Experience\", \"std_dev\"]].value_counts())\n",
    "```\n",
    "\n",
    "```\n",
    "salaries[\"median_by_comp_size\"] = salaries.groupby(\"Company_Size\")[\"Salary_USD\"].transform(lambda x: x.median())\n",
    "print(salaries[[\"Company_Size\", \"median_by_comp_size\"]].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca4d1a",
   "metadata": {},
   "source": [
    "## Handling outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef23206",
   "metadata": {},
   "source": [
    "### What is an outlier? Why look for them?\n",
    "\n",
    "- An outlier is an observation that is far away from other data points.\n",
    "- These are extreme values that may not accurately represent the data.\n",
    "- Additionally, they can skew the mean and standard deviation.\n",
    "\n",
    "If we plan to perform statistical tests or build machine learning models, these will often require data that is normally distributed and not skewed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86204655",
   "metadata": {},
   "source": [
    "### Using descriptive statistics\n",
    "\n",
    "- A starting place for identifying outliers is with the pandas dot-describe method.\n",
    "\n",
    "```\n",
    "print(salaries[\"Salary_USD\"].describe())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a015423",
   "metadata": {},
   "source": [
    "### IQR in boxplots\n",
    "\n",
    "- These percentiles are included in box plots, like this one showing salaries of data professionals.\n",
    "- The box contains percentiles, and observations considered to be outliers are represented as diamonds outside of the box.\n",
    "\n",
    "```\n",
    "sns.boxplot(data=salaries, y=\"Salaries_USD\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6e4c4e",
   "metadata": {},
   "source": [
    "### IQR, identifying threhsolds and oultiers\n",
    "\n",
    "- We can define an outlier mathematically.\n",
    "    1. First, we need to know the interquartile range (IQR) which is the difference between the 75th and 25th percentiles.\n",
    "    2. Once we have the IQR, we can find an upper outlier by looking for values above the sum of the 75th percentile plus one-point-five times the IQR.\n",
    "    3. Lower outliers have values below the sum of the 25th percentile minus one-point-five times the IQR.\n",
    "- We can calculate percentiles using the ``Series.quantile`` method.\n",
    "    1. We pass zero-point-seven-five to find the 75th percentile for salary, then pass zero-point-two-five to get the 25th percentile.\n",
    "    2. We calculate the IQR by subtracting one from the other.\n",
    "\n",
    "```\n",
    "# 75th percentile\n",
    "seventy_fifty = salaries[\"Salary_USD\"].quantile(0.75)\n",
    "\n",
    "# 25th percentile\n",
    "twenty_fifth = salaries[\"Salary_USD\"].quantile(0.25)\n",
    "\n",
    "# IQR\n",
    "iqr =  seventy_fifty - twenty_fifth\n",
    "\n",
    "print(iqr)\n",
    "\n",
    "# Upper threshold\n",
    "upper = seventy_fifty + (1.5*iqr)\n",
    "\n",
    "# Lower treshold\n",
    "lower = twenty_fifth - (1.5*iqr)\n",
    "\n",
    "print(upper, lower)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f888cc1d",
   "metadata": {},
   "source": [
    "### Dropping outliers\n",
    "\n",
    "- We can remove outliers by modifying the syntax we used to subset our data, filtering for values more than the lower limit and less than the upper limit.\n",
    "\n",
    "```\n",
    "no_outliers = salaries[(salaries[\"Salary_USD\"] > lower) & (salaries[\"Salary_USD\"] < upper)]\n",
    "print(no_outliers[\"Salary_USD\"].describe())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01859f",
   "metadata": {},
   "source": [
    "# 10.3 Relationships in data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33098c5",
   "metadata": {},
   "source": [
    "## Patterns over time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaa3a6f",
   "metadata": {},
   "source": [
    "### Importing TimeDate data\n",
    "\n",
    "- Before we can begin to look at potential patterns over time, we need to help pandas understand that data in a given column is in fact date or time data.\n",
    "- When a CSV file is imported into pandas, date and time data are typically interpreted as strings,\n",
    "- We can fix that by adding the parse_dates keyword argument to the CSV import and setting it equal to a list of column names that should be interpreted as DateTime data.\n",
    "\n",
    "```\n",
    "divorce = pd.read_csv(\"divorce.csv\", parse_dates=[\"marriage_data\"])\n",
    "divorce.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e15145",
   "metadata": {},
   "source": [
    "### Converting to DateTime data\n",
    "\n",
    "- We may wish to update data types to DateTime data after we import the data.\n",
    "- This is possible with ``pd.to_datetime()``, which converts the argument passed to it to DateTime data.\n",
    "\n",
    "```\n",
    "divorce[\"marriage_data\"] = pd.to_datetime(divorce[\"marriage_data\"])\n",
    "divorce.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca67814",
   "metadata": {},
   "source": [
    "### Create DateTime data\n",
    "\n",
    "- If a DataFrame has month, day, and year data stored in three different columns, as this one does, we can combine these columns into a single DateTime value by passing them to ``pd.to_datetime()``.\n",
    "- Note that for this trick to work, columns must be named \"month\", \"day\", and \"year\", but can appear in any order in the DataFrame.\n",
    "\n",
    "```\n",
    "divorce[\"marriage_data\"] = pd.to_datetime(divorce[[\"month\", \"day\", \"year\"]])\n",
    "divorce.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4d0656",
   "metadata": {},
   "source": [
    "### Create DateTime data\n",
    "\n",
    "- Extract parts of a full date using `dt.month`, `dt.day`, `dt.year`\n",
    "\n",
    "```\n",
    "divorce[\"marriage_month\"] = divorce[\"marriage_date\"].dt.month\n",
    "divorce.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f43ad75",
   "metadata": {},
   "source": [
    "### Visualizing patterns over time\n",
    "\n",
    "- Line plots are a great way to examine relationships between variables.\n",
    "- In Seaborn, line plots aggregate y values at each value of x and show the estimated mean and a confidence interval for that estimate.\n",
    "\n",
    "```\n",
    "sns.lineplot(data=divorce, x=\"marriage_month\", y=\"marriage_duration\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1458d",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9290509b",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "- Correlation describes the direction of the relationship between two variables as well as its strength.\n",
    "- Understanding this relationship can help us use variables to predict future outcomes\n",
    "- However, this highlights an important point about correlations: we must always interpret them within the context of our data\n",
    "\n",
    "```\n",
    "divorce.corr()\n",
    "\n",
    "divorce[\"divorce_date\"].min()\n",
    "divorce[\"divorce_date\"].max()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb37e3",
   "metadata": {},
   "source": [
    "### Correlation heatmaps\n",
    "\n",
    "- A heatmap has the benefit of color coding so that strong positive and negative correlations are easier to spot. \n",
    "- Setting the ``annot=True`` labels the correlation coefficient inside each cell.\n",
    "\n",
    "```\n",
    "sns.heatmap(divorce.corr(), annot=True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90383bc3",
   "metadata": {},
   "source": [
    "### Visualizing relationships\n",
    "\n",
    "- The Pearson coefficient we've been looking at only describes the linear correlation between variables.\n",
    "    - Variables can have a strong non-linear relationship and a Pearson correlation coefficient of close to zero.\n",
    "    - Alternatively, data might have a correlation coefficient indicating a strong linear relationship when another relationship, such as quadratic, is actually a better fit for the data.\n",
    "\n",
    "```\n",
    "sns.scatterplot(data=divorce, x=\"income_man\", y=\"income_woman\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "- We can take our scatterplots to the next level with Seaborn's pairplot. When passed a DataFrame, pairplot plots all pairwise relationships between numerical variables in one visualization.\n",
    "- We can limit the number of plotted relationships by setting the ``vars=[list_of_variables]`` argument equal to the variables of interest.\n",
    "\n",
    "```\n",
    "sns.pairplot(data=divorce, vars=[\"income_man\", \"income_woman\", \"marriage_duration\"])\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd7c8cc",
   "metadata": {},
   "source": [
    "## Factor relationships and distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebe372f",
   "metadata": {},
   "source": [
    "### Exploring categorical relationships\n",
    "\n",
    "- Categorical variables, or factors, also have relationships.\n",
    "- They are harder to summarize numerically, so we often rely on visualizations to explore their relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5709c907",
   "metadata": {},
   "source": [
    "### Kernel Density Estimate (KDE) plots\n",
    "\n",
    "- Similar to histograms, KDEs allow us to visualize distributions.\n",
    "- KDEs are considered more interpretable, though, especially when multiple distributions are shown as they are here.\n",
    "- However, due to the smoothing algorithm used in KDE plots, the curve can include values that don't make sense, so it's important to set good smoothing parameters.\n",
    "- To fix this, we can use the ``cut`` keyword argument. cut tells Seaborn how far past the minimum and maximum data values the curve should go when smoothing is applied.\n",
    "    - When we set cut equal to zero, the curve will be limited to values between the minimum and maximum x values\n",
    "\n",
    "```\n",
    "sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"education_man\", cut=0)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4c7a4",
   "metadata": {},
   "source": [
    "### Cummulative KDE plots\n",
    "\n",
    "- If we're interested in the cumulative distribution function, we can set the ``cumulative=True``.\n",
    "\n",
    "```\n",
    "sns.kdeplot(data=divorce, x=\"marriage_duration\", hue=\"education_man\", cut=0, cumulative=True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07475c07",
   "metadata": {},
   "source": [
    "# 10.4 Turning exploratory analysis into action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b3a98a",
   "metadata": {},
   "source": [
    "## Considerations for categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26cc6e7",
   "metadata": {},
   "source": [
    "### Categorical classes and class imbalance\n",
    "\n",
    "- With categorical data, one of the most important considerations is about the representation of classes, which is another term for labels.\n",
    "- Class imbalance happens when one class occurs more frequently than others. This can bias results, particularly if this class does not occur more frequently in the population."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc214f82",
   "metadata": {},
   "source": [
    "### Class frequency\n",
    "\n",
    "- Number of observations per class\n",
    "\n",
    "```\n",
    "print(planes[\"Destination\"].value_counts())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5086030",
   "metadata": {},
   "source": [
    "### Relative class frequency\n",
    "\n",
    "- We can use ``value_counts`` method again, but this time set the ``normalize=True`` keyword argument.\n",
    "- This returns the relative frequencies for each class.\n",
    "\n",
    "```\n",
    "print(planes[\"Destination\"].value_counts())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae22de1",
   "metadata": {},
   "source": [
    "### Cross-tabulation\n",
    "\n",
    "- Enables us to examine the frequency of combinations of classes.\n",
    "    1. Calling pandas.crosstab function.\n",
    "    2. Select the column to use as the index for the table\n",
    "    3. Select names of the columns in the table, and the values will be the count of combined observations.\n",
    "\n",
    "```\n",
    "pd.crosstab(planes[\"Source\"], planes[\"Destination\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156ea30",
   "metadata": {},
   "source": [
    "### Extending cross-tabulation\n",
    "\n",
    "- i.e. We can calculate the median price for these routes in our DataFrame, and compare the difference to these expected values.\n",
    "- We do this by adding two keyword arguments to ``pd.crosstab``.\n",
    "\n",
    "```\n",
    "pd.crosstab(planes[\"Source\"], planes[\"Destination\"], values=planes[\"Price\"], aggfunc=\"median\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b369b0a",
   "metadata": {},
   "source": [
    "## Generating new features\n",
    "\n",
    "Sometimes the format of our data can limit our ability to detect relationships or inhibit the potential performance of machine learning models.\n",
    "\n",
    "One method to overcome these issues is to generate new features from our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c43b6c",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "\n",
    "- Check correlation with a heatmap\n",
    "\n",
    "```\n",
    "sns.heatmap(planes.corr(), annot=True)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44b2716",
   "metadata": {},
   "source": [
    "### Data types\n",
    "\n",
    "- View the data types and establish which ones should be numerical\n",
    "\n",
    "```\n",
    "print(planes.dtypes)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bfc612",
   "metadata": {},
   "source": [
    "### Value counts\n",
    "\n",
    "- We see we need to i.e. remove string characters, and change non-stop to zero, before converting the data type to integer.\n",
    "\n",
    "```\n",
    "print(planes[\"Total_Stops\"].value_counts())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b123dfe6",
   "metadata": {},
   "source": [
    "### Cleaning a column\n",
    "\n",
    "- We use the ``string.replace`` method to first remove \" stops\", including the space, so that flights with two, three, or four stops are ready to convert.\n",
    "- Next we clean flights with one stop.\n",
    "- Lastly, we change \"non-stop\" to \"0\", then set the data type to integer.\n",
    "\n",
    "```\n",
    "planes[\"Total_Stops\"] = planes[\"Total_Stops\"].str.replace(\" stops\", \"\")\n",
    "planes[\"Total_Stops\"] = planes[\"Total_Stops\"].str.replace(\" stop\", \"\")\n",
    "planes[\"Total_Stops\"] = planes[\"Total_Stops\"].str.replace(\"non-stop\", \"0\")\n",
    "planes[\"Total_Stops\"] = planes[\"Total_Stops\"].astype(int)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df2466",
   "metadata": {},
   "source": [
    "### Extracting month and weekday\n",
    "\n",
    "- We know how to extract attributes from datetime values, so we can see if these offer any insights\n",
    "\n",
    "```\n",
    "planes[\"month\"] = planes[\"Date_of_Journey\"].dt.month\n",
    "planes[\"weekday\"] = planes[\"Date_of_Journey\"].dt.weekday\n",
    "print(planes[[\"month\", \"weekday\", \"Date_of_Journey\"]].head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8a00c",
   "metadata": {},
   "source": [
    "### ``pd.cut()``\n",
    "\n",
    "- Used to split out numeric data into categories\n",
    "\n",
    "```\n",
    "twenty_fifth = planes[\"Price\"].quantile(0.25)\n",
    "median = planes[\"Price\"].median()\n",
    "seventy_fifth = planes[\"Price\"].quantile(0.25)\n",
    "maximum = planes[\"Price\"].max()\n",
    "labels = [\"Economy\", \"Premium Economy\", \"Business Class\", \"First Class\"]\n",
    "bins = [0, twenty_fifth, median, seventy_fifth, maximum]\n",
    "planes[\"Price_Category\"] = pd.cut(planes[\"Price\"], labels=labels, bins=bins)\n",
    "print(planes[[\"Price\", \"Price_Category\"]].head())\n",
    "sns.countplot(data=planes, x=\"Airline\", hue=\"Price_Category\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a334ed2",
   "metadata": {},
   "source": [
    "## Generating hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd0fa07",
   "metadata": {},
   "source": [
    "### What is true?\n",
    "\n",
    "- When performing EDA, the question we should ask is: **how do we know what we are observing is true?**\n",
    "- To make conclusions regarding relationships, differences, and patterns in our data, we need to use a branch of statistics called **Hypothesis Testing**.\n",
    "- This involves the following steps before we even start collecting data:\n",
    "    1. Coming up with a hypothesis, or question\n",
    "    2. Specifying a statistical test that we will perform in order to reasonably conclude whether the hypothesis was true or false."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6ffea",
   "metadata": {},
   "source": [
    "### Data Snopping\n",
    "\n",
    "- The acts of excessive exploratory analysis, the generation of multiple hypotheses, and the execution of multiple statistical tests are collectively known as data snooping, or p-hacking.\n",
    "- Chances are, if we look at enough data and run enough tests, we will find a significant result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570e280",
   "metadata": {},
   "source": [
    "### Generating hypothesis\n",
    "\n",
    "- How do we generate hypotheses? We perform some EDA\n",
    "- Then, we design our experiment. It involves steps such as:\n",
    "    1. Choosing a sample\n",
    "    2. Calculating how many data points do we need\n",
    "    3. Deciding what statistical test to run"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
